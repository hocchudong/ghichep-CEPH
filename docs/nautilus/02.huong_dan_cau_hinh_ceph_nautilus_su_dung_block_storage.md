
# Hướng dẫn cấu hình CEPH Block Storage

## 1. Mô hình

![topo_ceph3node.png](./images/ceph_nautilus_topo.png)

## 2. IP Planning

![ip_planning_ceph03node.png](./images/ceph_nautilus_ip_planning.png)

## 3. Thực hiện

*Lưu ý:* Để thực hiện bài lab này, cần hoàn thành việc triển khai CEPH theo [tài liệu này](./01.huong_dan_cai_dat_ceph_nautilus.md)

- Thiết lập ip trên node `client1`

-  Đăng nhập với tài khoản root

```
su -
```

- Update OS

```sh
yum update -y
```

- Đặt hostname

```sh
hostnamectl set-hostname client1
```

- Cài đặt các gói phần mềm bổ trợ

```sh
yum install epel-release -y
yum update -y
yum install wget byobu curl git byobu python-setuptools python-virtualenv -y
```

- Đặt IP cho node `client1`

```sh
echo "Setup IP  ens32"
nmcli con modify ens32 ipv4.addresses 192.168.98.84/24
nmcli con modify ens32 ipv4.gateway 192.168.98.1
nmcli con modify ens32 ipv4.dns 8.8.8.8
nmcli con modify ens32 ipv4.method manual
nmcli con modify ens32 connection.autoconnect yes

echo "Setup IP  ens33"
nmcli con modify ens33 ipv4.addresses 192.168.62.84/24
nmcli con modify ens33 ipv4.method manual
nmcli con modify ens33 connection.autoconnect yes
```

-  Cấu hình các thành phần cơ bản

```sh
sudo systemctl disable firewalld
sudo systemctl stop firewalld
sudo systemctl disable NetworkManager
sudo systemctl stop NetworkManager
sudo systemctl enable network
sudo systemctl start network

sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/sysconfig/selinux
sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config
```

- Khai báo file `/etc/hosts`

```sh
cat << EOF > /etc/hosts
127.0.0.1 `hostname` localhost
192.168.62.84 client1
192.168.62.85 ceph1
192.168.62.86 ceph2
192.168.62.87 ceph3

192.168.98.84 client1
192.168.98.85 ceph1
192.168.98.86 ceph2
192.168.98.87 ceph3
EOF
```
	
- Khởi động lại node client 

```sh
init 6
```

##### Tạo user `cephuser`, khai báo repos cài đặt CEPH cho node `client1`
	
-  Đăng nhập lai node `client1` với IP mới `192.168.98.84`

- Cài đặt NTP

```
yum install -y chrony

systemctl enable chronyd.service
systemctl start chronyd.service
systemctl restart chronyd.service
chronyc sources
```

- Tạo user `cephuser` trên node `client1`. Mật khẩu của `cephuser` là `matkhau2019@`

```
useradd cephuser; echo 'matkhau2019@' | passwd cephuser --stdin
```

- Cấp quyền sudo cho tài khoản `cephuser`

```sh
echo "cephuser ALL = (root) NOPASSWD:ALL" | sudo tee /etc/sudoers.d/cephuser
chmod 0440 /etc/sudoers.d/cephuser
# sed -i s'/Defaults requiretty/#Defaults requiretty'/g /etc/sudoers
```

#### 5.2. Cài đặt các gói cho `client1`

###### Thực hiện trên node `ceph1`

- Chuyển sang user `cephuser`

```sh
su - cephuser
```

- Copy sshkey từ node `ceph1` sang node `cephclient`. Nhập password của user `cephuser` vừa tạo trên node `client1` ở bước trên.

```sh
ssh-copy-id cephuser@client1
```

- Di chuyển vào thư mục chứa các file cấu hình của ceph hoặc chuyển sang user `cephuser` để thực hiện các bước tiếp theo

```sh
cd /home/cephuser/my-cluster/
```

- Đứng trên node `ceph1` thực hiện cài đặt các gói cần thiết cho client.

```sh
ceph-deploy install --release nautilus client1
```

- Thực hiện deploy ceph cho node `client1`, bước này sẽ copy file `ceph.client.admin.keyring` từ node `ceph1` sang node `client1`.

```sh
ceph-deploy admin client1 
```

#### 5.3. Cài đặt ceph client cho node `client1`

Thực hiện trên node `client1`

- Phân quyền cho file `/etc/ceph/ceph.client.admin.keyring`

```sh
sudo chmod +r /etc/ceph/ceph.client.admin.keyring
```

#### 5.4. Cấu hình RDB cho client sử dụng.

##### Thực hiện trên node `ceph1`

- Khai báo pool tên là `rbd` để client sử dụng. Theo tài liệu gốc thì khuyến cáo nên đặt tên là `rbd` vì mặc định khi ta tạo các `images` trong CEPH thì nó sẽ nằm ở pool có tên là `rdb`. Còn nếu muốn các images nằm ở các pools khác thì trong lệnh tạo RBD images cần có thêm tùy chọn  `-p`.

```sh
ceph osd pool create rbd 128
```

- Khai báo pool có tên là `rdb` vừa tạo ở trên được sử dụng bởi RDB của CEPH.

```sh
rbd pool init rbd
```

- Kiểm tra pool vừa tạo xem đã có hay chưa bằng lệnh  `ceph osd lspools` hoặc `ceph osd pool ls` 

```sh
ceph osd lspools
```

hoặc

```
ceph osd pool ls
```

- Kết quả ta sẽ thấy pool có tên là `rbd` vừa được tạo ở trên.

- Kiểm tra xem có images nào trong pool `rbd` hay không bằng lệnh `rbd ls ten_pool`. Kết quả trả về sẽ không có images nào.

```
rbd ls rbd
```

- Hoặc chỉ cần gõ `rbd ls` và không cần chỉ ra tên pools, CEPH sẽ tự lấy tên là `rbd`.




##### Thực hiện trên node `client1`

- Đứng trên node `client1` thực hiện tạo một image có tên là `disk01` với dung lượng là 10GB, image này sẽ nằm trong pool có tên là `rdb` vừa tạo ở trên. Nếu bạn muốn images này nằm ở pool có tên khác thì cần thêm tùy chọn `-p ten_pools` trong lệnh dưới.

```sh
rbd create disk01 --size 10G --image-feature layering
```
	
- Hoặc lệnh với tùy chọn chỉ định pools như sau
	
```sh
rbd create disk01 --size 10G -p ten_pool --image-feature layering
```

- Có thể đứng ngày ở `client1` và dùng lệnh liệt kê các images để kiểm tra lại xem các images RDB đã được tạo hay chưa

```sh
rbd ls -l
```
	
- Kết quả

```sh
[root@client1 ~]#  rbd ls -l
NAME    SIZE PARENT FMT PROT LOCK
disk01 10GiB          2
[root@client1 ~]#
```
		
- Thực hiện map images đã được tạo tới một disk của máy client

```sh
rbd map disk01 
```
	
- Kết quả của lệnh trên

```sh
[root@client1 ~]# rbd map disk01
/dev/rbd0
```
	
- Lệnh trên sẽ thực hiện map images có tên là `disk01` tới một thiết bị trên client, thiết bị này sẽ được đặt tên là `/dev/rdbX`. Trong đó `X` sẽ bắt đầu từ 0 và tăng dần lên. Nếu muốn biết về việc quản lý thiết bị trong linux thì đọc thêm các tài liệu của Linux nhé bạn đọc ơi.

- Thực hiện kiểm tra xem images RDB có tên là `disk01` đã được map hay chưa.

```sh
rbd showmapped
```

- Kết quả: 

```sh
[root@client1 ~]# rbd showmapped
id pool image  snap device
0  rbd  disk01 -    /dev/rbd0
```

- Hoặc bằng các lệnh khác để kiểm tra ổ đĩa trong linux như: `lsblk`

- Kết quả:
        
    ```sh
    [root@client1 ~]# lsblk
    NAME                    MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
    fd0                       2:0    1    4K  0 disk
    sda                       8:0    0   80G  0 disk
    ├─sda1                    8:1    0    4G  0 part /boot
    ├─sda2                    8:2    0   68G  0 part
    │ └─VolGroup00-LogVol01 253:0    0   68G  0 lvm  /
    └─sda3                    8:3    0    8G  0 part [SWAP]
    sr0                      11:0    1 1024M  0 rom
    rbd0                    252:0    0   10G  0 disk
    ```
    
- Tới đây máy client chưa	thể sử dụng ổ được map vì chưa được phân vùng, tiếp tục thực hiện bước phân vùng và mount vào một thư mục nào đó để sử dụng. Thời gian thực hiện lệnh dưới sẽ cần chờ từ 10-30 giây.

```sh
sudo mkfs.xfs /dev/rbd0
```

- Kết quả: 

```sh
[root@client1 ~]# sudo mkfs.xfs /dev/rbd0
meta-data=/dev/rbd0              isize=512    agcount=16, agsize=163840 blks
                    =                       sectsz=512   attr=2, projid32bit=1
                    =                       crc=1        finobt=0, sparse=0
data     =                       bsize=4096   blocks=2621440, imaxpct=25
                    =                       sunit=1024   swidth=1024 blks
naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
log      =internal log           bsize=4096   blocks=2560, version=2
                    =                       sectsz=512   sunit=8 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0
[root@client1 ~]#
```

- Thực hiện mount vào thư mục `/mtn`

```sh
sudo mount /dev/rbd0 /mnt
```

- Kiểm tra lại xem đã mount được hay chưa

```sh
df -hT 
```	
	
- Kết quả:

```sh
[root@client1 ~]# df -hT
Filesystem                      Type      Size  Used Avail Use% Mounted on
/dev/mapper/VolGroup00-LogVol01 xfs        68G  1.6G   67G   3% /
devtmpfs                        devtmpfs  989M     0  989M   0% /dev
tmpfs                           tmpfs    1000M     0 1000M   0% /dev/shm
tmpfs                           tmpfs    1000M  8.9M  992M   1% /run
tmpfs                           tmpfs    1000M     0 1000M   0% /sys/fs/cgroup
/dev/sda1                       ext4      3.9G  174M  3.5G   5% /boot
tmpfs                           tmpfs     200M     0  200M   0% /run/user/0
/dev/rbd0                       xfs        10G   33M   10G   1% /mnt
```

- Hoặc kiểm tra bằng lệnh `lsblk`, ta sẽ có kết quả như bên dưới. Lúc này ta có thể ghi dữ liệu vào ổ `/mnt` để sử dụng.

```sh
[root@client1 ~]# lsblk
NAME                    MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
fd0                       2:0    1    4K  0 disk
sda                       8:0    0   80G  0 disk
├─sda1                    8:1    0    4G  0 part /boot
├─sda2                    8:2    0   68G  0 part
│ └─VolGroup00-LogVol01 253:0    0   68G  0 lvm  /
└─sda3                    8:3    0    8G  0 part [SWAP]
sr0                      11:0    1 1024M  0 rom
rbd0                    252:0    0   10G  0 disk /mnt
[root@client1 ~]#
```

*Lưu ý:* Vì mount  chưa được khai báo trong `fstab` nên khi khởi động lại máy client thì thao tác mount này sẽ bị mất. Thực hiện khai báo trong `fstab` như sau:.

- Kích hoạt `rbd map` bằng lệnh dưới.

```
systemctl start rbdmap

systemctl enable rbdmap
```

Kiểm tra lại trạng thái của `rbdmap`, ta có kết quả như bên dưới.

```
● rbdmap.service - Map RBD devices
   Loaded: loaded (/usr/lib/systemd/system/rbdmap.service; enabled; vendor preset: disabled)
   Active: active (exited) since Wed 2020-02-05 21:31:45 +07; 9min ago
  Process: 1064 ExecStart=/usr/bin/rbdmap map (code=exited, status=0/SUCCESS)
 Main PID: 1064 (code=exited, status=0/SUCCESS)
   CGroup: /system.slice/rbdmap.service

Feb 05 21:31:44 client1 systemd[1]: Starting Map RBD devices...
Feb 05 21:31:44 client1 rbdmap[1069]: Mapping 'rbd/disk01'
Feb 05 21:31:45 client1 rbdmap[1142]: Mapped 'rbd/disk01' to '/dev/rbd0'
Feb 05 21:31:45 client1 rbdmap[1164]: Mounted '/dev/rbd0' to 'mount: /dev/rbd0 mounted on /mnt.'
Feb 05 21:31:45 client1 systemd[1]: Started Map RBD devices.
```

Sửa file `vi /etc/ceph/rbdmap` bằng việc thêm dòng `rbd/disk01    id=admin,keyring=/etc/ceph/ceph.client.admin.keyring` vào cuối file. Nội dung của file `/etc/ceph/rbdmap` như dưới.

```
[root@client1 ~]# cat /etc/ceph/rbdmap
# RbdDevice             Parameters
#poolname/imagename     id=client,keyring=/etc/ceph/ceph.client.keyring
rbd/disk01    id=admin,keyring=/etc/ceph/ceph.client.admin.keyring
```

Trong đó: 
- `rbd/disk01:` dược khai báo theo cú pháp `ten_pool/ten_image`
- `id:` là giá trị `admin`
- `keyring:` là đường dẫn file key được tạo ở bước trước đó.

Sửa file `/etc/fstab` bằng việc khai báo đường dẫn của device và thư mục được mount. Thêm dòng dưới vào cuối cùng của file.

```
/dev/rbd/rbd/disk01             /mnt                    xfs     noauto          0 0
```

Nội dung của file /etc/fstab sẽ có dạng như sau

```
[root@client1 ~]# cat /etc/fstab
#
# /etc/fstab
# Created by anaconda on Sun Sep  2 16:46:37 2018
#
# Accessible filesystems, by reference, are maintained under '/dev/disk'
# See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info
#
/dev/mapper/centos-root /                       xfs     defaults        0 0
UUID=a03e4ac4-d684-4e31-b8d6-93fbdf6d8d02 /boot                   xfs     defaults        0 0
/dev/mapper/centos-home /home                   xfs     defaults        0 0
/dev/mapper/centos-swap swap                    swap    defaults        0 0
/dev/rbd/rbd/disk01             /mnt                    xfs     noauto          0 0
```

Đường dẫn của device sẽ có dạng: `/dev/rbd/ten_pool/ten_image`. Trong ví dụ này chính là `/dev/rbd/rbd/disk01` và `/mnt` là thư mục được mount.

Sau khi khai báo xong, có thể khởi động lại và kiểm tra bằng lệnh `lsblk` để xem `disk01` còn được gắn vào hay không, nếu không thì hãy kiểm tra lại các bước trên :).

_HẾT_